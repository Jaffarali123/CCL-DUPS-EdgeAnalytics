import math

import torch
import numpy as np
from COMA import COMA
import matplotlib.pyplot as plt

#
from environment import Environment
from collections import Counter
from actor_critic import ACAgent

def moving_average(x, N):
    return np.convolve(x, np.ones(N, np.float32) / N, mode='valid')

def counts(actions):
    turn_on_ctr = 0
    turn_off_ctr = 0
    for action in actions:
        for a in action:
            if a == 0:
                turn_off_ctr += 1
            else:
                turn_on_ctr += 1

    return turn_off_ctr, turn_on_ctr

if __name__ == '__main__':
    agent_num = 132
    state_dim = 4
    action_dim = 2
    hidden_size = 512

    gamma = 0.99
    lr_a = 0.00001
    lr_c = 0.000005

    target_update_steps = 24
    agents = COMA(agent_num, state_dim, action_dim, lr_c, lr_a, gamma, target_update_steps)

    env = Environment()

    episode_reward = 0
    episodes_reward = []
    num_episodes = 200
    num_steps = 24

    for episode in range(num_episodes):
        print(f"--------------------- Episode: {episode} ---------------------")
        obs = env.get_state()
        # print("obs::", obs)
        total_episode_reward = []
        print(range(num_steps))
        policy_arrays = [[] for _ in range(24)]
        best_policies = [[] for _ in range(24)]

        for step in range(24):
            env.set_date_and_time(date=episode, time=step)
            actions = agents.get_actions(obs)

            # actions_for_step = actions[step]
            # print("Actions for step {}: {}".format(step, actions_for_step))

            # Convert boolean tensors to integers
            # actions_for_step = [int(action) for action in actions_for_step]

            # print("Actions for step", step, ":", actions_for_step)
            # print(f"Turned on: {sum(actions_for_step)} --- Turned Off: {len(actions_for_step) - sum(actions_for_step)}")
            # print(f"Step: {step}/{num_steps}")
            #
            # print("Length of actions for step {}: {}".format(step, len(actions_for_step)))

            next_obs, reward, done_n = env.step(time_y=step, actions=actions)

            # print("Next observation is: ")
            # for i in range(len(next_obs)):
            #     print(f"BS: {i} --- observation is: {next_obs[i]}")

            # for obs in next_obs:
            #     print(obs)

            # total_turned_on = counts(actions)[1] * 100
            # difference = total_turned_on - total_traffic_before_action
            # print(f"Time: {step} ---- deficit: {difference}")
            # Get the policy distributions generated by the actors for this step
            # policies = [torch.softmax(agents.actors[j](next_obs[j]), dim=-1) for j in range(agents.agent_num)]

            # print("Policies: ", policies)

            # Store the best policies for this step
            # for j, policy in enumerate(policies):
            #     action_chosen = 1 if policy[0] < policy[1] else 0
            #     best_policies[step].append(action_chosen)

            # # Print the policy distributions for this step
            # print(f"Policies at step {step}:")
            # for j, policy in enumerate(policies):
            #     action_chosen = 1 if policy[0] < policy[1] else 0
            #     print(f"Agent {j} Policy: {action_chosen}")
            #
            # # Store the policies for this step
            # for j in range(agents.agent_num):
            #     policy_arrays[step].append(policies[j])


            # print(f"time: {step} --- traffic: {current_traffic}")
            agents.memory.reward.append(reward)
            for i in range(agent_num):
                agents.memory.done[i].append(done_n[i])

            episode_reward += sum(reward)
            # print("Episode is: ", episode_reward)
            obs = next_obs
            # if all(done_n):
            episodes_reward.append(episode_reward)
            episode_reward = 0
            agents.train()
            print(f"Episode: {episode}, Average reward: {sum(episodes_reward[-100:]) / 100}")
                # print(episodes_reward)

            # steps_with_op_policy = []
            # for j, policy in enumerate(policies):
            #     action_chosen = 1 if policy[0] < policy[1] else 0
            #     best_policies[step].append(action_chosen)
            #
            # if 0 < env.penalty <= 2.0:
            #     print(f"Optimal policy found at day: {env.date} --- time: {env.time} \n Policy vector: {best_policies}")
            #     steps_with_op_policy.append([env.date, env.time])
    # print()
    # Initialize variables
    # action_sets = []
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    # action_sets = [[] for _ in range(24)]  # List to store actions for each step
    # episodes_reward = []
    #
    # num_episodes = 2
    # num_steps = 24

    # for episode in range(num_episodes):
    #     print(f"--------------------- Episode: {episode} ---------------------")
    #     obs = env.get_state()
    #     total_episode_reward = []
    #
    #     for step in range(num_steps):
    #         actions = agents.get_actions(obs)
    #
    #         if episode == num_episodes - 1:  # Store actions only for the last episode
    #             action_sets[step].append(actions)  # Store actions for each step
    #
    #         print(f"Step: {step}/{num_steps}")
    #
    #         next_obs, reward, done_n = env.step(time_y=step, actions=actions)
    #         agents.memory.reward.append(reward)
    #
    #         for i in range(agent_num):
    #             agents.memory.done[i].append(done_n[i])
    #
    #         episode_reward += sum(reward)
    #         obs = next_obs
    #
    #         if all(done_n):
    #             episodes_reward.append(episode_reward)
    #             episode_reward = 0
    #             agents.train()
    #             print(f"Episode: {episode}, Average reward: {sum(episodes_reward[-100:]) / 100}")

    # Display the actions taken at each step of the final episode
    # print("\nActions taken at each step of the final episode:")
    # for step, actions in enumerate(action_sets):
    #     print(f"Step {step}: {actions}")
        #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    # Print the best policies learned by the agents for each time step
    # for step, policies in enumerate(best_policies):
    #     print(f"Policies at step {step}: {policies}")
    N = len(episodes_reward)
    running_avg = np.empty(N)
    for t in range(N):
        running_avg[t] = np.mean(episodes_reward[max(0, t - 100):(t + 1)])

    x = [(i + 1) for i in range(len(episodes_reward))]
    plt.plot(x, running_avg)
    plt.title("Total reward per episode (online)")
    plt.ylabel("reward")
    plt.xlabel("episode")
    plt.show()


